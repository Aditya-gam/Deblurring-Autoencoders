{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder CelebA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-09T21:37:14.166546Z",
     "iopub.status.busy": "2024-03-09T21:37:14.166084Z",
     "iopub.status.idle": "2024-03-09T21:37:22.260482Z",
     "shell.execute_reply": "2024-03-09T21:37:22.259457Z",
     "shell.execute_reply.started": "2024-03-09T21:37:14.166506Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "# import cv2\n",
    "\n",
    "# from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "# from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T21:37:22.263537Z",
     "iopub.status.busy": "2024-03-09T21:37:22.262718Z",
     "iopub.status.idle": "2024-03-09T21:37:22.291521Z",
     "shell.execute_reply": "2024-03-09T21:37:22.290659Z",
     "shell.execute_reply.started": "2024-03-09T21:37:22.263495Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to your image\n",
    "image_path = './data/CelebA/test/blur/10.jpg'\n",
    "\n",
    "# Open the image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Get the size of the image\n",
    "size = image.size\n",
    "\n",
    "# Print the size (Width, Height)\n",
    "print(\"Image Size: \", size)\n",
    "\n",
    "# To get the shape, especially if you plan to work with PyTorch, you'll likely need the channels as well\n",
    "# PIL Images are in Width x Height format. For the shape, we usually want channels as well,\n",
    "# which for RGB images is 3. PyTorch uses Channels x Height x Width format.\n",
    "shape = (3, image.height, image.width)\n",
    "\n",
    "# Print the shape\n",
    "print(\"Image Shape: \", shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T21:37:22.292760Z",
     "iopub.status.busy": "2024-03-09T21:37:22.292482Z",
     "iopub.status.idle": "2024-03-09T21:37:22.301602Z",
     "shell.execute_reply": "2024-03-09T21:37:22.300674Z",
     "shell.execute_reply.started": "2024-03-09T21:37:22.292720Z"
    }
   },
   "outputs": [],
   "source": [
    "class CelebADeblurDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            split (string): One of 'train', 'validation', 'test' to select the dataset split.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = os.path.join(root_dir, split)\n",
    "        self.transform = transform\n",
    "        self.blur_dir = os.path.join(self.root_dir, 'blur')\n",
    "        self.sharp_dir = os.path.join(self.root_dir, 'sharp')\n",
    "        self.image_files = os.listdir(self.blur_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        blur_img_path = os.path.join(self.blur_dir, self.image_files[idx])\n",
    "        sharp_img_path = os.path.join(self.sharp_dir, self.image_files[idx])\n",
    "\n",
    "        blur_image = Image.open(blur_img_path).convert('RGB')\n",
    "        sharp_image = Image.open(sharp_img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            blur_image = self.transform(blur_image)\n",
    "            sharp_image = self.transform(sharp_image)\n",
    "\n",
    "        return blur_image, sharp_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T21:37:22.303612Z",
     "iopub.status.busy": "2024-03-09T21:37:22.303338Z",
     "iopub.status.idle": "2024-03-09T21:37:24.548093Z",
     "shell.execute_reply": "2024-03-09T21:37:24.547266Z",
     "shell.execute_reply.started": "2024-03-09T21:37:22.303586Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define your transforms (adjust as necessary)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the images to 256x256\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Instantiate your dataset\n",
    "root_dir = './data/CelebA'\n",
    "train_dataset = CelebADeblurDataset(root_dir=root_dir, split='train', transform=transform)\n",
    "validation_dataset = CelebADeblurDataset(root_dir=root_dir, split='validation', transform=transform)\n",
    "test_dataset = CelebADeblurDataset(root_dir=root_dir, split='test', transform=transform)\n",
    "\n",
    "# Load the datasets into DataLoader for efficient batching and shuffling\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T21:37:24.550545Z",
     "iopub.status.busy": "2024-03-09T21:37:24.550248Z",
     "iopub.status.idle": "2024-03-09T21:37:26.309513Z",
     "shell.execute_reply": "2024-03-09T21:37:26.308444Z",
     "shell.execute_reply.started": "2024-03-09T21:37:24.550521Z"
    }
   },
   "outputs": [],
   "source": [
    "def imshow_grid(blurred_images, sharp_images, n_images=5, title=None):\n",
    "    \"\"\"Function to show n pairs of blurred and sharp images\"\"\"\n",
    "    # Assuming `blurred_images` and `sharp_images` are batches of images\n",
    "    # Select the first `n_images`\n",
    "    blurred_images = blurred_images[:n_images]\n",
    "    sharp_images = sharp_images[:n_images]\n",
    "    \n",
    "    # Make a grid for blurred and sharp images\n",
    "    blurred_grid = torchvision.utils.make_grid(blurred_images, nrow=n_images)\n",
    "    sharp_grid = torchvision.utils.make_grid(sharp_images, nrow=n_images)\n",
    "    \n",
    "    # Convert to numpy arrays for plotting\n",
    "    np_blurred_grid = blurred_grid.numpy()\n",
    "    np_sharp_grid = sharp_grid.numpy()\n",
    "    \n",
    "    # Concatenate grids along height (axis=1 for Height in HWC format after transpose)\n",
    "    combined_grid = np.concatenate((np.transpose(np_blurred_grid, (1, 2, 0)), np.transpose(np_sharp_grid, (1, 2, 0))), axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))  # Adjust size as needed\n",
    "    plt.imshow(combined_grid)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "# To visualize the first few pairs of images\n",
    "blurred_images, sharp_images = next(iter(train_loader))\n",
    "\n",
    "# Visualizing the first image pair as an example\n",
    "imshow_grid(blurred_images, sharp_images, n_images=5, title=\"Blurred vs Sharp Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T21:37:26.311194Z",
     "iopub.status.busy": "2024-03-09T21:37:26.310892Z",
     "iopub.status.idle": "2024-03-09T21:37:26.333204Z",
     "shell.execute_reply": "2024-03-09T21:37:26.332090Z",
     "shell.execute_reply.started": "2024-03-09T21:37:26.311169Z"
    }
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = self.init_encoder()\n",
    "        self.decoder = self.init_decoder()\n",
    "\n",
    "    def init_encoder(self):\n",
    "        \"\"\"\n",
    "        Initializes the encoder part of the autoencoder with 4 convolutional layers.\n",
    "        \"\"\"\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),  # Output: (16, 128, 128)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # Output: (32, 64, 64)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # Output: (64, 32, 32)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), # Output: (128, 16, 16)\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        return encoder\n",
    "\n",
    "    def init_decoder(self):\n",
    "        \"\"\"\n",
    "        Initializes the decoder part of the autoencoder with 4 convolutional layers.\n",
    "        \"\"\"\n",
    "        decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1), # Output: (64, 32, 32)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: (32, 64, 64)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: (16, 128, 128)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: (3, 256, 256)\n",
    "            nn.Sigmoid()  # Use sigmoid to scale the output to [0,1]\n",
    "        )\n",
    "        return decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, train_loader, validation_loader, epochs=10, learning_rate=1e-3, weight_decay=1e-5, loss_fn=nn.MSELoss(), verbose=False):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.to(device)\n",
    "        results = []  # List to store outputs for visualization\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.train()  # Set model to training mode\n",
    "            train_loss = 0.0\n",
    "            for data in train_loader:\n",
    "                inputs, targets = data\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "\n",
    "            if verbose:\n",
    "                self.eval()  # Set the model to evaluation mode\n",
    "                with torch.no_grad():  # Inference mode, gradient not computed\n",
    "                    validation_loss = 0.0\n",
    "                    for data in validation_loader:\n",
    "                        inputs, targets = data\n",
    "                        inputs, targets = inputs.to(device), targets.to(device)\n",
    "                        outputs = self(inputs)\n",
    "                        loss = loss_fn(outputs, targets)\n",
    "                        validation_loss += loss.item()\n",
    "                    validation_loss /= len(validation_loader)\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}\")\n",
    "            \n",
    "            # Store outputs along with epoch number for visualization\n",
    "            if epoch % 4 == 0:  # Adjust as per your requirement\n",
    "                results.append((epoch, inputs.cpu(), outputs.cpu()))\n",
    "                \n",
    "        return outputs\n",
    "\n",
    "    def visualize_reconstructions(self, outputs, n=5):\n",
    "        \"\"\"\n",
    "        Visualizes original and reconstructed images.\n",
    "        \"\"\"\n",
    "        for epoch, imgs, recons in outputs:\n",
    "            plt.figure(figsize=(10, 2))\n",
    "            for i in range(n):\n",
    "                if i >= len(imgs): break\n",
    "                plt.subplot(2, n, i + 1)\n",
    "                plt.imshow(imgs[i].permute(1, 2, 0))\n",
    "                plt.axis('off')\n",
    "                \n",
    "                plt.subplot(2, n, n + i + 1)\n",
    "                plt.imshow(recons[i].permute(1, 2, 0))\n",
    "                plt.axis('off')\n",
    "            plt.suptitle(f'Epoch: {epoch}')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().to(device)\n",
    "model.load_state_dict(torch.load('./autoencoder_model_full8.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T21:37:26.334821Z",
     "iopub.status.busy": "2024-03-09T21:37:26.334429Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 20  # You can adjust the number of epochs\n",
    "outputs = model.fit(train_loader, validation_loader, epochs=num_epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "model_save_path = './autoencoder_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the reconstructions\n",
    "model.visualize_reconstructions(outputs[-1], n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr_pytorch(img1, img2, data_range=1.0):\n",
    "    mse = F.mse_loss(img1, img2)\n",
    "    psnr_val = 20 * torch.log10(data_range / torch.sqrt(mse))\n",
    "    return psnr_val.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_pytorch(img1, img2, data_range=1.0, C1=6.5025, C2=58.5225):\n",
    "    mu1 = img1.mean([2, 3])\n",
    "    mu2 = img2.mean([2, 3])\n",
    "    sigma1_sq = ((img1 - mu1.unsqueeze(-1).unsqueeze(-1)) ** 2).mean([2, 3])\n",
    "    sigma2_sq = ((img2 - mu2.unsqueeze(-1).unsqueeze(-1)) ** 2).mean([2, 3])\n",
    "    sigma12 = ((img1 - mu1.unsqueeze(-1).unsqueeze(-1)) * (img2 - mu2.unsqueeze(-1).unsqueeze(-1))).mean([2, 3])\n",
    "\n",
    "    ssim_num = (2 * mu1 * mu2 + C1) * (2 * sigma12 + C2)\n",
    "    ssim_den = (mu1 ** 2 + mu2 ** 2 + C1) * (sigma1_sq + sigma2_sq + C2)\n",
    "    ssim_score = ssim_num / ssim_den\n",
    "\n",
    "    return ssim_score.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_mse, total_psnr, total_ssim = 0, 0, 0\n",
    "    criterion = nn.MSELoss()\n",
    "    for batch in test_loader:\n",
    "        inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        mse = criterion(outputs, targets).item()\n",
    "        total_mse += mse\n",
    "        \n",
    "        # Compute PSNR and SSIM for each item in the batch and accumulate\n",
    "        batch_psnr = psnr_pytorch(outputs, targets, data_range=1.0)\n",
    "        batch_ssim = ssim_pytorch(outputs, targets, data_range=1.0)\n",
    "        total_psnr += batch_psnr * outputs.size(0)  # Multiply by batch size because psnr_pytorch returns the average\n",
    "        total_ssim += batch_ssim * outputs.size(0)  # Multiply by batch size because ssim_pytorch returns the average\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_mse = total_mse / len(test_loader.dataset)\n",
    "    avg_psnr = total_psnr / len(test_loader.dataset)\n",
    "    avg_ssim = total_ssim / len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Test MSE: {avg_mse:.4f}, PSNR: {avg_psnr:.4f}, SSIM: {avg_ssim:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3055596,
     "sourceId": 5251537,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
