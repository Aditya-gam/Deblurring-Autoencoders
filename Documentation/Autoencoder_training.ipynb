{"cells":[{"cell_type":"markdown","metadata":{"id":"N_DE_ZOrxrS1"},"source":["# Importing libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CaNkAtYYxrS5"},"outputs":[],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Dataset\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Device: {device}')"]},{"cell_type":"markdown","metadata":{"id":"JUQgNwkGxrS7"},"source":["# Function definition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_lcT8vPJxrS7"},"outputs":[],"source":["#Dataset\n","class CelebADeblurDataset(Dataset):\n","    def __init__(self, root_dir, split='train', transform=None):\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory with all the images.\n","            split (string): One of 'train', 'validation', 'test' to select the dataset split.\n","            transform (callable, optional): Optional transform to be applied on a sample.\n","        \"\"\"\n","        self.root_dir = os.path.join(root_dir, split)\n","        self.transform = transform\n","        self.blur_dir = os.path.join(self.root_dir, 'blur')\n","        self.sharp_dir = os.path.join(self.root_dir, 'sharp')\n","        self.image_files = os.listdir(self.blur_dir)[:50000]\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        blur_img_path = os.path.join(self.blur_dir, self.image_files[idx])\n","        sharp_img_path = os.path.join(self.sharp_dir, self.image_files[idx])\n","\n","        blur_image = Image.open(blur_img_path).convert('RGB')\n","        sharp_image = Image.open(sharp_img_path).convert('RGB')\n","\n","        if self.transform:\n","            blur_image = self.transform(blur_image)\n","            sharp_image = self.transform(sharp_image)\n","\n","        return blur_image, sharp_image\n","\n","\n","#Visualising images\n","def imshow_grid(blurred_images, sharp_images, n_images=5, title=None):\n","    \"\"\"Function to show n pairs of blurred and sharp images\"\"\"\n","    # Assuming `blurred_images` and `sharp_images` are batches of images\n","    # Select the first `n_images`\n","    blurred_images = blurred_images[:n_images]\n","    sharp_images = sharp_images[:n_images]\n","\n","    # Make a grid for blurred and sharp images\n","    blurred_grid = torchvision.utils.make_grid(blurred_images, nrow=n_images)\n","    sharp_grid = torchvision.utils.make_grid(sharp_images, nrow=n_images)\n","\n","    # Convert to numpy arrays for plotting\n","    np_blurred_grid = blurred_grid.numpy()\n","    np_sharp_grid = sharp_grid.numpy()\n","\n","    # Concatenate grids along height (axis=1 for Height in HWC format after transpose)\n","    combined_grid = np.concatenate((np.transpose(np_blurred_grid, (1, 2, 0)), np.transpose(np_sharp_grid, (1, 2, 0))), axis=0)\n","\n","    plt.figure(figsize=(15, 6))  # Adjust size as needed\n","    plt.imshow(combined_grid)\n","    if title:\n","        plt.title(title)\n","    plt.axis('off')\n","    plt.show()\n","\n","\n","\n","#Autoencoder model\n","class Autoencoder(nn.Module):\n","    def __init__(self):\n","        super(Autoencoder, self).__init__()\n","        self.encoder = self.init_encoder()\n","        self.decoder = self.init_decoder()\n","\n","    def init_encoder(self):\n","        \"\"\"\n","        Initializes the encoder part of the autoencoder with 4 convolutional layers.\n","        \"\"\"\n","        encoder = nn.Sequential(\n","            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),  # Output: (16, 128, 128)\n","            nn.ReLU(True),\n","            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # Output: (32, 64, 64)\n","            nn.ReLU(True),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # Output: (64, 32, 32)\n","            nn.ReLU(True),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), # Output: (128, 16, 16)\n","            nn.ReLU(True),\n","        )\n","        return encoder\n","\n","    def init_decoder(self):\n","        \"\"\"\n","        Initializes the decoder part of the autoencoder with 4 convolutional layers.\n","        \"\"\"\n","        decoder = nn.Sequential(\n","            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1), # Output: (64, 32, 32)\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: (32, 64, 64)\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: (16, 128, 128)\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: (3, 256, 256)\n","            nn.Sigmoid()  # Use sigmoid to scale the output to [0,1]\n","        )\n","        return decoder\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","    def fit(self, train_loader, validation_loader, epochs=10, learning_rate=1e-3, weight_decay=1e-5, loss_fn=nn.MSELoss(), verbose=False):\n","        optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","        self.to(device)\n","        results = []  # List to store outputs for visualization\n","\n","        for epoch in range(epochs):\n","            self.train()  # Set model to training mode\n","            train_loss = 0.0\n","            for data in train_loader:\n","                inputs, targets = data\n","                inputs, targets = inputs.to(device), targets.to(device)\n","\n","                optimizer.zero_grad()\n","                outputs = self(inputs)\n","                loss = loss_fn(outputs, targets)\n","                loss.backward()\n","                optimizer.step()\n","\n","                train_loss += loss.item()\n","\n","            train_loss /= len(train_loader)\n","\n","            if verbose:\n","                self.eval()  # Set the model to evaluation mode\n","                with torch.no_grad():  # Inference mode, gradient not computed\n","                    validation_loss = 0.0\n","                    for data in validation_loader:\n","                        inputs, targets = data\n","                        inputs, targets = inputs.to(device), targets.to(device)\n","                        outputs = self(inputs)\n","                        loss = loss_fn(outputs, targets)\n","                        validation_loss += loss.item()\n","                    validation_loss /= len(validation_loader)\n","\n","                print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}\")\n","\n","            # Store outputs along with epoch number for visualization\n","            if epoch % 4 == 0:  # Adjust as per your requirement\n","                results.append((epoch, inputs.cpu(), outputs.cpu()))\n","\n","        return outputs\n","\n","    def visualize_reconstructions(self, outputs, n=5):\n","        \"\"\"\n","        Visualizes original and reconstructed images.\n","        \"\"\"\n","        for epoch, imgs, recons in outputs:\n","            plt.figure(figsize=(10, 2))\n","            for i in range(n):\n","                if i >= len(imgs): break\n","                plt.subplot(2, n, i + 1)\n","                plt.imshow(imgs[i].permute(1, 2, 0))\n","                plt.axis('off')\n","\n","                plt.subplot(2, n, n + i + 1)\n","                plt.imshow(recons[i].permute(1, 2, 0))\n","                plt.axis('off')\n","            plt.suptitle(f'Epoch: {epoch}')\n","            plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CSTb8OpSxrS8"},"source":["# Loading our dataset into dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lKlNL79oxrS8"},"outputs":[],"source":["# Define your transforms (adjust as necessary)\n","transform = transforms.Compose([\n","    transforms.Resize((256, 256)),  # Resize the images to 256x256\n","    transforms.ToTensor(),\n","])\n","\n","# # Instantiate your dataset\n","root_dir = '/home/pdutta/myenv/dataset_occ/CelebA'\n","train_dataset = CelebADeblurDataset(root_dir=root_dir, split='train', transform=transform)\n","validation_dataset = CelebADeblurDataset(root_dir=root_dir, split='validation', transform=transform)\n","test_dataset = CelebADeblurDataset(root_dir=root_dir, split='test', transform=transform)\n","\n","# Load the datasets into DataLoader for efficient batching and shuffling\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","num_batches = len(train_loader)  # Number of batches\n","batch_size = train_loader.batch_size  # Batch size\n","\n","# Total number of images\n","# This calculation assumes that all batches are full, except possibly the last one.\n","total_images = num_batches * batch_size//1000\n","print(total_images*1000)"]},{"cell_type":"markdown","metadata":{"id":"o_WLtawzxrS9"},"source":["# Training and saving model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3aAqSoQPxrS9"},"outputs":[],"source":["# Instantiate the autoencoder model\n","model = Autoencoder.to(device)\n","\n","# Train the model\n","num_epochs = 20  # You can adjust the number of epochs\n","outputs = model.fit(train_loader, validation_loader, epochs=num_epochs, verbose=True)\n","\n","# Save the model's state dictionary\n","model_save_path = './autoencoder_model_full8.pth'\n","torch.save(model.state_dict(), model_save_path)"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}